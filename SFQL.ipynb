{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qeuYjM5-87z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gLey0OjT-87-"
   },
   "outputs": [],
   "source": [
    "##Environment\n",
    "class GridWorld:\n",
    "    def __init__(self, height = 50, width = 50, n_c = 1, n_o = 1):\n",
    "        self.wd = width\n",
    "        self.ht = height\n",
    "        self.n_c = n_c            # no. of classes\n",
    "        self.n_o = n_o            # no. of objects  (n_c <= n_o)\n",
    "        self.reward = [-1, 1]     # range of rewards\n",
    "        self.start_coord = (0, 0)\n",
    "        self.goal_coord = (height-1, width-1)\n",
    "        self.x_g = height-1        # goal x-coord\n",
    "        self.x_g = width-1         # goal y-coord\n",
    "        self.r_g = 1               # goal reward\n",
    "        self.D = 9 + n_o\n",
    "        self.d = n_c + 1\n",
    "        '''\n",
    "        UP: 0\n",
    "        RIGHT: 1\n",
    "        DOWN: 2\n",
    "        LEFT: 3\n",
    "        '''\n",
    "        self.actions = [0, 1, 2, 3]\n",
    "        #self.x_s, self.y_s = self.start_coord\n",
    "\n",
    "        self.object_coord = []\n",
    "        self.object_class = []\n",
    "        self.class_reward = {}\n",
    "        self.objects_so_far = [0] * self.n_o\n",
    "\n",
    "        self.generate_objects()    # Done once\n",
    "        \n",
    "        self.assign_classes()\n",
    "        self.generate_class_reward()\n",
    "\n",
    "    def generate_objects(self):\n",
    "        n_o = self.n_o\n",
    "        while n_o:\n",
    "            object_x = np.random.randint(self.wd)\n",
    "            object_y = np.random.randint(self.ht)\n",
    "            if((object_x,object_y) not in self.object_coord):\n",
    "                self.object_coord.append((object_x,object_y))\n",
    "                n_o -= 1\n",
    "  \n",
    "    def assign_classes (self):\n",
    "        self.object_class = np.random.randint(0, self.n_c, size = self.n_o)\n",
    " \n",
    "    def generate_class_reward(self):\n",
    "        for i in range(0, self.n_c):\n",
    "            self.class_reward[i] = np.random.uniform(self.reward[0], self.reward[1])\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_pos_x, self.cur_pos_y = self.start_coord\n",
    "        self.object_so_far = [0] * self.n_o\n",
    "        self.generate_class_reward()\n",
    "        return self.start_coord\n",
    "\n",
    "    def render(self):\n",
    "        for i in range(self.wd):\n",
    "            for j in range(self.ht):\n",
    "                if (i, j) in self.object_coord:\n",
    "                    print(self.object_class[self.object_coord.index((i, j))], end = \"\")\n",
    "                else:\n",
    "                    print(\"*\",end = \"\")\n",
    "            print(\"\")\n",
    "            \n",
    "    def show_class_rewards(self):\n",
    "        print(self.class_reward)\n",
    "    \n",
    "    def restart(self):\n",
    "        self.cur_pos_x, self.cur_pos_y = self.start_coord\n",
    "        self.objects_so_far = [0] * self.n_o\n",
    "        return self.start_coord\n",
    "        \n",
    "    def step(self, action):\n",
    "        flag = 1\n",
    "        reward = 0\n",
    "        phi_at_step = [0] * (self.n_c + 1)\n",
    "        done = False\n",
    "        if action == 0 and self.cur_pos_y + 1 < self.ht:\n",
    "            self.cur_pos_y += 1\n",
    "        elif action == 1 and self.cur_pos_x + 1 < self.wd:\n",
    "            self.cur_pos_x += 1\n",
    "        elif action == 2 and self.cur_pos_y - 1 >=0:\n",
    "            self.cur_pos_y -= 1\n",
    "        elif action == 3 and self.cur_pos_x - 1 >=0:\n",
    "            self.cur_pos_x -= 1\n",
    "        \n",
    "        pos = (self.cur_pos_x, self.cur_pos_y)\n",
    "        if pos == self.goal_coord:\n",
    "            phi_at_step[-1] = self.r_g\n",
    "            done = True\n",
    "            reward += 1\n",
    "        elif pos in self.object_coord:\n",
    "            object_index = self.object_coord.index(pos)\n",
    "            if self.objects_so_far[object_index] == 0:\n",
    "                phi_at_step[self.object_class[object_index]] = 1\n",
    "                self.objects_so_far[object_index] = 1\n",
    "                reward += self.class_reward[self.object_class[object_index]]\n",
    "\n",
    "        return pos, np.array(phi_at_step), reward, done\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld(10, 10, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "1******1**\n",
      "***0******\n",
      "**********\n",
      "*1********\n",
      "0*********\n",
      "******00*1\n",
      "*1********\n",
      "**********\n",
      "****1*****\n",
      "State (1, 0) PHI [0, 1, 0] Reward -0.5561393869552584 State (5, 0) PHI [1, 0, 0] Reward -0.5280895023193286 State (4, 1) PHI [0, 1, 0] Reward -0.5561393869552584 State (7, 1) PHI [0, 1, 0] Reward -0.5561393869552584 State (6, 6) PHI [1, 0, 0] Reward -0.5280895023193286 State (6, 7) PHI [1, 0, 0] Reward -0.5280895023193286 State (1, 7) PHI [0, 1, 0] Reward -0.5561393869552584 State (6, 9) PHI [0, 1, 0] Reward -0.5561393869552584 State (9, 9) PHI [0, 0, 1] Reward 1 "
     ]
    }
   ],
   "source": [
    "done = False\n",
    "state = env.restart()\n",
    "env.render()\n",
    "#env.showClassRewards()\n",
    "while not done:\n",
    "    a = np.random.choice([0, 1, 2, 3])\n",
    "    state, phi, r, done = env.step(a)\n",
    "    if r != 0:\n",
    "        print(\"State\", state, end = \" \")\n",
    "        print(\"PHI\", phi, end = \" \")\n",
    "        print(\"Reward\", r, end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zex7rD5HevzT"
   },
   "outputs": [],
   "source": [
    "class RadialBasis():\n",
    "\n",
    "    def __init__(self, x_dim, y_dim, basis_x, basis_y):\n",
    "\n",
    "        self.centres = []\n",
    "        self.x_dim, self.y_dim = x_dim, y_dim\n",
    "        for x in np.linspace(0, x_dim, basis_x):\n",
    "            for y in np.linspace(0, y_dim, basis_y):\n",
    "                self.centres.append((x/x_dim, y/y_dim))\n",
    "      \n",
    "    def getPositionVector(self, x, y):\n",
    "        state = []\n",
    "        x,y = x/self.x_dim, y/self.y_dim\n",
    "        for cx, cy in self.centres:\n",
    "            state.append(np.exp(-1 * ((cx - x)**2 + (cy - y)**2)/0.1))\n",
    "        return np.array(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OMwb80ZW-88E"
   },
   "outputs": [],
   "source": [
    "class SFQL:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.reward_weight_list = []\n",
    "        self.Z_list = []\n",
    "        self.eps_greedy = 0.4\n",
    "        self.gamma = 0.95\n",
    "        self.w_alpha = 0.01\n",
    "        self.z_alpha = 0.01\n",
    "        self.w_err_th = 0.01\n",
    "        self.rdb = RadialBasis(env.wd, env.ht, 3, 3)\n",
    "\n",
    "    def featurize_state(self, state):\n",
    "        return np.hstack((self.rdb.getPositionVector(state[0], state[1]), np.array(self.env.objects_so_far)))\n",
    "    \n",
    "    def find_best_psi(self, w_t, state):\n",
    "        max_k = 0\n",
    "        action_val = -999999\n",
    "        action_val_his = -999999\n",
    "        for k in range(0,len(self.Z_list)):\n",
    "            for action in self.env.actions: \n",
    "                psi = np.dot(self.featurize_state(state).T, self.Z_list[k][action])\n",
    "                new_action_val = np.dot(psi.T,w_t)\n",
    "                action_val = max(action_val,new_action_val)\n",
    "            max_k = k if (action_val > action_val_his) else max_k\n",
    "            action_val_his = action_val\n",
    "        \n",
    "        return max_k \n",
    "                \n",
    "    def get_action(self, state, c, w_t):\n",
    "        if np.random.uniform(0,1) < self.eps_greedy:   #In paper, Bernoulli is considered imstead of uniform\n",
    "            return np.random.choice(self.env.actions)\n",
    "        else:\n",
    "            val_his = -99999\n",
    "            act_choice = 0\n",
    "            for action in self.env.actions:\n",
    "                psi = np.dot(self.featurize_state(state).T, self.Z_list[c][action])\n",
    "                val = np.dot(psi.T,w_t)\n",
    "                act_choice = action if val > val_his else act_choice\n",
    "                val_his = max(val_his,val)\n",
    "            return act_choice\n",
    "\n",
    "\n",
    "    \n",
    "    def algorithm(self,num_tasks):\n",
    "        D = self.env.D\n",
    "        d = self.env.d\n",
    "\n",
    "        NUM_EPISODES = 100\n",
    "        \n",
    "        #TODO: How to initialize the list? add before train or after train (only for the first task)\n",
    "        \n",
    "        Z = [np.random.rand(D,d) for r in range(len(self.env.actions))]\n",
    "       \n",
    "        for t in range(0, num_tasks):\n",
    "            print(\"Task: \",t)\n",
    "        \n",
    "            w_t = np.random.rand(d)\n",
    "            self.Z_list.append(deepcopy(Z))\n",
    "            self.reward_weight_list.append(deepcopy(w_t))\n",
    "            Z = self.Z_list[-1]\n",
    "            w_t = self.reward_weight_list[-1]\n",
    "            env.reset()\n",
    "\n",
    "            mavg = 0\n",
    "            for ep in range(NUM_EPISODES):\n",
    "                gamma = self.gamma\n",
    "                state = self.env.restart()\n",
    "                done = False\n",
    "                creward = 0\n",
    "                while not done:\n",
    "                    c = self.find_best_psi(w_t,state)\n",
    "                    w_c = self.reward_weight_list[c]\n",
    "                    action = self.get_action(state, c, w_t)\n",
    "                    s_prime, phi_at_step, reward, done = self.env.step(action)\n",
    "                    if done:\n",
    "                        gamma = 0\n",
    "                    else:\n",
    "                        c_prime = self.find_best_psi(w_t, s_prime)\n",
    "                        a_prime = self.get_action(s_prime, c_prime, w_t)\n",
    "                    creward += reward\n",
    "                    \n",
    "                    w_t = w_t + self.w_alpha * (reward - np.dot(phi_at_step.T, w_t)) * phi_at_step\n",
    "                    psi_prime = np.dot(self.featurize_state(s_prime).T, self.Z_list[t][a_prime])\n",
    "                    psi = np.dot(self.featurize_state(state).T, self.Z_list[t][action])\n",
    "                    z_t = self.Z_list[t][action]\n",
    "                    for k in range(0,d):\n",
    "                        target_k = phi_at_step[k] + gamma*psi_prime[k]\n",
    "                        z_t[:, k] = z_t[:, k] + self.z_alpha * (target_k - psi[k]) * self.featurize_state(state)\n",
    "\n",
    "                    if c != t:\n",
    "                        a_prime = self.get_action(s_prime, c, w_c)\n",
    "\n",
    "                        psi_prime = np.dot(self.featurize_state(s_prime).T, self.Z_list[c][a_prime])\n",
    "                        psi = np.dot(self.featurize_state(state).T, self.Z_list[c][action])\n",
    "                        z_c = self.Z_list[c][action]\n",
    "                        for k in range(0,d):\n",
    "                            target_k = phi_at_step[k] + gamma*psi_prime[k]\n",
    "                            z_c[:, k] = z_c[:, k] + self.z_alpha * (target_k - psi[k]) * self.featurize_state(state)\n",
    "\n",
    "                    state = s_prime\n",
    "                mavg += creward\n",
    "                print(round(mavg/(ep + 1),4), end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LsCH3-2ueuQd",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task:  0\n",
      "-0.2523 0.3002 0.116 0.0239 0.2412 0.3524 0.4028 0.4337 0.3575 0.3168 0.3571 0.3401 0.3526 0.3344 0.3149 0.3324 0.3305 0.3288 0.3428 0.3306 0.3554 0.3504 0.3482 0.3631 0.3584 0.3774 0.3854 0.3804 0.3758 0.3867 0.3839 0.3905 0.3878 0.3939 0.3954 0.401 0.4132 0.402 0.4017 0.3913 0.3815 0.3844 0.3871 0.378 0.3841 0.3754 0.3672 0.3731 0.3693 0.3639 0.3706 0.3653 0.3679 0.3646 0.3708 0.3748 0.3716 0.3729 0.3767 0.3788 0.3832 0.3768 0.3765 0.3848 0.3787 0.3783 0.3725 0.3728 0.373 0.3675 0.3674 0.3741 0.3743 0.3773 0.3802 0.3804 0.3832 0.3859 0.3783 0.3831 0.3863 0.3815 0.3847 0.3885 0.3886 0.3886 0.3887 0.387 0.3871 0.3878 0.39 0.3885 0.3907 0.3885 0.3806 0.3786 0.3746 0.3748 0.3729 0.3751 Task:  1\n",
      "2.4828 2.4828 2.4828 2.7299 2.5816 2.5651 2.5534 2.6063 2.5926 2.6805 2.6625 2.769 2.785 2.8992 2.9044 2.878 2.8257 2.8341 2.8636 2.8693 2.8944 2.897 2.9219 2.9417 2.9431 2.9444 2.9639 2.9644 2.9648 2.9652 2.9656 2.9798 2.9498 2.9651 2.9506 2.9513 2.952 2.926 2.914 2.9026 2.9044 2.9055 2.9072 2.9194 2.9092 2.9101 2.9215 2.9221 2.9328 2.9332 2.9341 2.9534 2.9347 2.9264 2.9448 2.9273 2.9277 2.92 2.9042 2.9046 2.9058 2.899 2.9002 2.8937 2.8794 2.8734 2.8671 2.8542 2.8417 2.8295 2.8312 2.8264 2.8281 2.8231 2.8185 2.8141 2.8095 2.8113 2.8134 2.8093 2.7991 2.8013 2.8034 2.8055 2.8017 2.7977 2.7938 2.7903 2.7921 2.7829 2.785 2.7868 2.7889 2.7909 2.7876 2.7844 2.7864 2.7884 2.7853 2.7823 Task:  2\n",
      "1.2535 0.7016 0.8856 0.8925 0.812 0.7583 0.7199 0.6911 0.6688 0.6339 0.6902 0.6593 0.6201 0.6411 0.6196 0.6009 0.5843 0.5696 0.5877 0.6125 0.643 0.6631 0.6407 0.6521 0.6693 0.6918 0.7126 0.6925 0.6797 0.6988 0.6811 0.699 0.7441 0.7316 0.7514 0.7346 0.7579 0.7709 0.7876 0.7993 0.8104 0.8209 0.827 0.8328 0.846 0.8548 0.8597 0.8679 0.8758 0.8681 0.8606 0.8502 0.8434 0.851 0.8414 0.8487 0.8528 0.8466 0.8564 0.8531 0.8416 0.8331 0.8425 0.8317 0.8382 0.8329 0.8392 0.8453 0.8512 0.8436 0.8362 0.8314 0.8267 0.8222 0.8234 0.8168 0.8103 0.816 0.8119 0.8333 0.8312 0.8363 0.8434 0.8371 0.8331 0.8271 0.83 0.8262 0.831 0.8357 0.8384 0.8346 0.8373 0.8399 0.846 0.8441 0.8501 0.8481 0.8428 0.8503 Task:  3\n",
      "1.8692 1.6519 1.6786 1.549 1.6131 1.6196 1.6552 1.6548 1.6786 1.6977 1.6935 1.6967 1.6933 1.7214 1.7168 1.7127 1.7091 1.706 1.7188 1.7263 1.7266 1.7268 1.7236 1.7239 1.721 1.7184 1.7159 1.7214 1.7293 1.7267 1.7198 1.7177 1.7223 1.7266 1.7245 1.7285 1.7264 1.723 1.7212 1.7194 1.7164 1.72 1.7203 1.7255 1.7287 1.7318 1.7271 1.7256 1.7257 1.7199 1.7201 1.712 1.715 1.7153 1.7156 1.7183 1.7186 1.7249 1.7274 1.7274 1.7226 1.7215 1.7217 1.724 1.7262 1.7284 1.7317 1.7317 1.7337 1.7325 1.7325 1.7325 1.7325 1.7314 1.7285 1.7275 1.7265 1.7256 1.7301 1.7292 1.7238 1.7213 1.7231 1.7232 1.7223 1.7215 1.7232 1.7199 1.7192 1.7208 1.7201 1.7178 1.7171 1.7164 1.7158 1.7151 1.7144 1.7146 1.7148 1.7163 Task:  4\n",
      "-1.3546 -1.3546 -1.5509 -1.5705 -1.5273 -1.3481 -1.349 -1.3105 -1.25 -1.2879 -1.1869 -1.1257 -1.098 -1.0939 -1.0328 -1.0529 -1.0868 -1.1497 -1.1295 -1.125 -1.0799 -1.0656 -1.0526 -1.0407 -1.0642 -1.0754 -1.0421 -1.0421 -1.0623 -1.0027 -0.9761 -0.9328 -0.9539 -0.9045 -0.9252 -0.8957 -0.8763 -0.8961 -0.8847 -0.8739 -0.8425 -0.8473 -0.818 -0.8302 -0.8479 -0.8649 -0.8503 -0.824 -0.8404 -0.8562 -0.8483 -0.8294 -0.8222 -0.859 -0.8623 -0.8865 -0.9043 -0.9067 -0.889 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-8ed505c3328b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridWorld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSFQL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-87-a6691c89969f>\u001b[0m in \u001b[0;36malgorithm\u001b[0;34m(self, num_tasks)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mcreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_best_psi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                     \u001b[0mw_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_weight_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-a6691c89969f>\u001b[0m in \u001b[0;36mfind_best_psi\u001b[0;34m(self, w_t, state)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mpsi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeaturize_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0mnew_action_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0maction_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_action_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-a6691c89969f>\u001b[0m in \u001b[0;36mfeaturize_state\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeaturize_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetPositionVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjects_so_far\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_best_psi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \"\"\"\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0;31m# As a special case, dimension 0 of 1-dimensional arrays is \"horizontal\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \"\"\"\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0;31m# As a special case, dimension 0 of 1-dimensional arrays is \"horizontal\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = GridWorld(10, 10, 2, 10)\n",
    "a = SFQL(env)\n",
    "a.algorithm(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zx13n_rYeuPR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVZ8o-xT-88I"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SFQL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
