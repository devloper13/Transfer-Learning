{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qeuYjM5-87z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gLey0OjT-87-"
   },
   "outputs": [],
   "source": [
    "##Environment\n",
    "class GridWorld:\n",
    "    def __init__(self, height = 50, width = 50, n_c = 1, n_o = 1):\n",
    "        self.wd = width\n",
    "        self.ht = height\n",
    "        self.n_c = n_c            # no. of classes\n",
    "        self.n_o = n_o            # no. of objects  (n_c <= n_o)\n",
    "        self.reward = [-1, 1]     # range of rewards\n",
    "        self.start_coord = (0, 0)\n",
    "        self.goal_coord = (height-1, width-1)\n",
    "        self.x_g = height-1        # goal x-coord\n",
    "        self.x_g = width-1         # goal y-coord\n",
    "        self.r_g = 1               # goal reward\n",
    "        self.D = 100 + n_o\n",
    "        self.d = n_c + 1\n",
    "        '''\n",
    "        UP: 0\n",
    "        RIGHT: 1\n",
    "        DOWN: 2\n",
    "        LEFT: 3\n",
    "        '''\n",
    "        self.actions = [0, 1, 2, 3]\n",
    "        #self.x_s, self.y_s = self.start_coord\n",
    "\n",
    "        self.object_coord = []\n",
    "        self.object_class = []\n",
    "        self.class_reward = {}\n",
    "        self.objects_so_far = [0] * self.n_o\n",
    "\n",
    "        self.generate_objects()    # Done once\n",
    "        \n",
    "        self.assign_classes()\n",
    "        self.generate_class_reward()\n",
    "\n",
    "    def generate_objects(self):\n",
    "        n_o = self.n_o\n",
    "        while n_o:\n",
    "            object_x = np.random.randint(self.wd)\n",
    "            object_y = np.random.randint(self.ht)\n",
    "            if((object_x,object_y) not in self.object_coord):\n",
    "                self.object_coord.append((object_x,object_y))\n",
    "                n_o -= 1\n",
    "  \n",
    "    def assign_classes (self):\n",
    "        self.object_class = np.random.randint(0, self.n_c, size = self.n_o)\n",
    " \n",
    "    def generate_class_reward(self):\n",
    "        for i in range(0, self.n_c):\n",
    "            self.class_reward[i] = np.random.uniform(self.reward[0], self.reward[1])\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_pos_x, self.cur_pos_y = self.start_coord\n",
    "        self.object_so_far = [0] * self.n_o\n",
    "        self.generate_class_reward()\n",
    "        return self.start_coord\n",
    "\n",
    "    def render(self):\n",
    "        for i in range(self.wd):\n",
    "            for j in range(self.ht):\n",
    "                if (i, j) in self.object_coord:\n",
    "                    print(self.object_class[self.object_coord.index((i, j))], end = \"\")\n",
    "                else:\n",
    "                    print(\"*\",end = \"\")\n",
    "            print(\"\")\n",
    "            \n",
    "    def show_class_rewards(self):\n",
    "        print(self.class_reward)\n",
    "    \n",
    "    def restart(self):\n",
    "        self.cur_pos_x, self.cur_pos_y = self.start_coord\n",
    "        self.objects_so_far = [0] * self.n_o\n",
    "        return self.start_coord\n",
    "        \n",
    "    def step(self, action):\n",
    "        flag = 1\n",
    "        reward = 0\n",
    "        phi_at_step = [0] * (self.n_c + 1)\n",
    "        done = False\n",
    "        if action == 0 and self.cur_pos_y + 1 < self.ht:\n",
    "            self.cur_pos_y += 1\n",
    "        elif action == 1 and self.cur_pos_x + 1 < self.wd:\n",
    "            self.cur_pos_x += 1\n",
    "        elif action == 2 and self.cur_pos_y - 1 >=0:\n",
    "            self.cur_pos_y -= 1\n",
    "        elif action == 3 and self.cur_pos_x - 1 >=0:\n",
    "            self.cur_pos_x -= 1\n",
    "        \n",
    "        pos = (self.cur_pos_x, self.cur_pos_y)\n",
    "        if pos == self.goal_coord:\n",
    "            phi_at_step[-1] = self.r_g\n",
    "            done = True\n",
    "            reward += 1\n",
    "        elif pos in self.object_coord:\n",
    "            object_index = self.object_coord.index(pos)\n",
    "            if self.objects_so_far[object_index] == 0:\n",
    "                phi_at_step[self.object_class[object_index]] = 1\n",
    "                self.objects_so_far[object_index] = 1\n",
    "                reward += self.class_reward[self.object_class[object_index]]\n",
    "\n",
    "        return pos, np.array(phi_at_step), reward, done\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld(10, 10, 3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*****1***\n",
      "**********\n",
      "********0*\n",
      "1*0*******\n",
      "*0***0***2\n",
      "**********\n",
      "*********2\n",
      "***2******\n",
      "**********\n",
      "**********\n",
      "State (3, 0) PHI [0 1 0 0] Reward 0.37621542260936347 State (7, 3) PHI [0 0 1 0] Reward -0.47600063034601825 State (4, 1) PHI [1 0 0 0] Reward 0.9037764071377654 State (0, 0) PHI [0 0 1 0] Reward -0.47600063034601825 State (3, 2) PHI [1 0 0 0] Reward 0.9037764071377654 State (4, 5) PHI [1 0 0 0] Reward 0.9037764071377654 State (2, 8) PHI [1 0 0 0] Reward 0.9037764071377654 State (0, 6) PHI [0 1 0 0] Reward 0.37621542260936347 State (9, 9) PHI [0 0 0 1] Reward 1 "
     ]
    }
   ],
   "source": [
    "done = False\n",
    "state = env.restart()\n",
    "env.render()\n",
    "#env.showClassRewards()\n",
    "while not done:\n",
    "    a = np.random.choice([0, 1, 2, 3])\n",
    "    state, phi, r, done = env.step(a)\n",
    "    if r != 0:\n",
    "        print(\"State\", state, end = \" \")\n",
    "        print(\"PHI\", phi, end = \" \")\n",
    "        print(\"Reward\", r, end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zex7rD5HevzT"
   },
   "outputs": [],
   "source": [
    "class RadialBasis():\n",
    "\n",
    "    def __init__(self, x_dim, y_dim, basis_x, basis_y):\n",
    "\n",
    "        self.centres = []\n",
    "        self.x_dim, self.y_dim = x_dim, y_dim\n",
    "        for x in np.linspace(0, x_dim, basis_x):\n",
    "            for y in np.linspace(0, y_dim, basis_y):\n",
    "                self.centres.append((x/x_dim, y/y_dim))\n",
    "      \n",
    "    def getPositionVector(self, x, y):\n",
    "        state = []\n",
    "        x,y = x/self.x_dim, y/self.y_dim\n",
    "        for cx, cy in self.centres:\n",
    "            state.append(np.exp(-1 * ((cx - x)**2 + (cy - y)**2)/0.1))\n",
    "        return np.array(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OMwb80ZW-88E"
   },
   "outputs": [],
   "source": [
    "class SFQL:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.reward_weight_list = []\n",
    "        self.Z_list = []\n",
    "        self.eps_greedy = 0.15\n",
    "        self.gamma = 0.95\n",
    "        self.num_steps = 10000\n",
    "        self.w_alpha = 0.01\n",
    "        self.z_alpha = 0.01\n",
    "        self.rdb = RadialBasis(env.wd, env.ht, 10, 10)\n",
    "\n",
    "    def featurize_state(self, state):\n",
    "        return np.hstack((self.rdb.getPositionVector(state[0], state[1]), np.array(self.env.objects_so_far)))\n",
    "    \n",
    "    def find_best_psi(self, w_t, small_phi):\n",
    "        max_k = 0\n",
    "        action_val = -999999\n",
    "        action_val_his = -999999\n",
    "        for k in range(0,len(self.Z_list)):\n",
    "            for action in self.env.actions: \n",
    "                psi = np.dot(small_phi.T, self.Z_list[k][action])\n",
    "                new_action_val = np.dot(psi.T,w_t)\n",
    "                action_val = max(action_val,new_action_val)\n",
    "            max_k = k if (action_val > action_val_his) else max_k\n",
    "            action_val_his = action_val\n",
    "        \n",
    "        return max_k \n",
    "                \n",
    "    def get_action(self, small_phi, c, w_t):\n",
    "        if np.random.uniform(0,1) < self.eps_greedy:   #In paper, Bernoulli is considered imstead of uniform\n",
    "            return np.random.choice(self.env.actions)\n",
    "        else:\n",
    "            val_his = -99999\n",
    "            act_choice = 0\n",
    "            for action in self.env.actions:\n",
    "                psi = np.dot(small_phi.T, self.Z_list[c][action])\n",
    "                val = np.dot(psi.T,w_t)\n",
    "                act_choice = action if val > val_his else act_choice\n",
    "                val_his = max(val_his,val)\n",
    "            return act_choice\n",
    "\n",
    "    \n",
    "    \n",
    "    def algorithm(self,num_tasks):\n",
    "        D = self.env.D\n",
    "        d = self.env.d\n",
    "        cum_reward_list = []\n",
    "        NUM_STEPS = self.num_steps\n",
    "        \n",
    "        #TODO: How to initialize the list? add before train or after train (only for the first task)\n",
    "        \n",
    "        Z = [np.random.rand(D,d) for r in range(len(self.env.actions))]\n",
    "        \n",
    "       \n",
    "        for t in range(0, num_tasks):\n",
    "            print(\"Task: \",t)\n",
    "        \n",
    "            w_t = np.random.rand(d)\n",
    "            #print(\"Z = \",Z)\n",
    "            self.Z_list.append(deepcopy(Z))\n",
    "            self.reward_weight_list.append(deepcopy(w_t))\n",
    "            Z = self.Z_list[-1]\n",
    "            w_t = self.reward_weight_list[-1]\n",
    "            env.reset()\n",
    "\n",
    "            cum_reward = 0\n",
    "            new_episode = True\n",
    "            for ep in range(NUM_STEPS):\n",
    "                if new_episode == True:\n",
    "                    gamma = self.gamma\n",
    "                    state = self.env.restart()\n",
    "                    done = False\n",
    "                    #creward = 0\n",
    "                    new_episode = False\n",
    "                \n",
    "                small_phi = self.featurize_state(state)\n",
    "                \n",
    "                c = self.find_best_psi(w_t,small_phi)\n",
    "                w_c = self.reward_weight_list[c]\n",
    "                action = self.get_action(small_phi, c, w_t)\n",
    "                s_prime, phi_at_step, reward, done = self.env.step(action)\n",
    "\n",
    "                \n",
    "                small_phi_prime = self.featurize_state(s_prime)\n",
    "\n",
    "                if done:\n",
    "                    gamma = 0\n",
    "                    new_episode = True\n",
    "                else:\n",
    "                    c_prime = self.find_best_psi(w_t, small_phi_prime)\n",
    "                    a_prime = self.get_action(small_phi_prime, c_prime, w_t)\n",
    "                \n",
    "                cum_reward += reward\n",
    "                cum_reward_list.append(cum_reward)\n",
    "                w_t = w_t + self.w_alpha * (reward - np.dot(phi_at_step.T, w_t)) * phi_at_step\n",
    "                psi_prime = np.dot(small_phi_prime.T, self.Z_list[t][a_prime])\n",
    "                psi = np.dot(small_phi.T, self.Z_list[t][action])\n",
    "                z_t = self.Z_list[t][action]\n",
    "                for k in range(0,d):\n",
    "                    target_k = phi_at_step[k] + gamma*psi_prime[k]\n",
    "                    z_t[:, k] = z_t[:, k] + self.z_alpha * (target_k - psi[k]) * small_phi\n",
    "\n",
    "                if c != t:\n",
    "                    a_prime = self.get_action(small_phi_prime, c, w_c)\n",
    "\n",
    "                    psi_prime = np.dot(small_phi_prime.T, self.Z_list[c][a_prime])\n",
    "                    psi = np.dot(small_phi.T, self.Z_list[c][action])\n",
    "                    z_c = self.Z_list[c][action]\n",
    "                    for k in range(0,d):\n",
    "                        target_k = phi_at_step[k] + gamma*psi_prime[k]\n",
    "                        z_c[:, k] = z_c[:, k] + self.z_alpha * (target_k - psi[k]) * small_phi_prime\n",
    "\n",
    "                state = s_prime\n",
    "                #print(round(cum_reward,4),end = \" \")\n",
    "            \n",
    "            #mavg += creward\n",
    "            print(\"Final\",round(cum_reward,4))\n",
    "        \n",
    "        self.plot(cum_reward_list,num_tasks)\n",
    "    \n",
    "    def plot(self,cum_reward_list,num_talks):\n",
    "        x_size = self.num_steps * num_talks\n",
    "        x_axis = list(range(1,x_size+1))\n",
    "        y_axis = cum_reward_list\n",
    "        plt.plot(x_axis,y_axis)\n",
    "        plt.xlabel(\"steps\")\n",
    "        plt.ylabel(\"cumulative reward\")\n",
    "        plt.title(\"SFQL\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LsCH3-2ueuQd",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task:  0\n",
      "Final 4.917\n",
      "Task:  1\n",
      "Final 40.4663\n",
      "Task:  2\n",
      "Final 0.8612\n",
      "Task:  3\n",
      "Final 8.7448\n",
      "Task:  4\n",
      "Final 0.3936\n",
      "Task:  5\n",
      "Final 1.0317\n",
      "Task:  6\n",
      "Final 0.1739\n",
      "Task:  7\n",
      "Final 7.8028\n",
      "Task:  8\n",
      "Final 18.587\n",
      "Task:  9\n",
      "Final 213.6867\n",
      "Task:  10\n",
      "Final 9.3576\n",
      "Task:  11\n",
      "Final 75.3996\n",
      "Task:  12\n",
      "Final 0.0092\n",
      "Task:  13\n",
      "Final 0.0162\n",
      "Task:  14\n",
      "Final 0.4768\n",
      "Task:  15\n",
      "Final -0.7802\n",
      "Task:  16\n",
      "Final 3.1813\n",
      "Task:  17\n",
      "Final 13.2698\n",
      "Task:  18\n",
      "Final 0\n",
      "Task:  19\n",
      "Final 3.0116\n",
      "Task:  20\n",
      "Final 0.8419\n",
      "Task:  21\n",
      "Final -0.782\n",
      "Task:  22\n",
      "Final 1.3585\n",
      "Task:  23\n",
      "Final 2.3578\n",
      "Task:  24\n",
      "Final 0.0278\n",
      "Task:  25\n",
      "Final 0.2403\n",
      "Task:  26\n",
      "Final 0.5503\n",
      "Task:  27\n",
      "Final 16.5752\n",
      "Task:  28\n",
      "Final -2.9857\n",
      "Task:  29\n",
      "Final 3.9712\n",
      "Task:  30\n",
      "Final 6.327\n",
      "Task:  31\n",
      "Final 183.9445\n",
      "Task:  32\n",
      "Final 72.3905\n",
      "Task:  33\n",
      "Final 299.8371\n",
      "Task:  34\n",
      "Final 357.212\n",
      "Task:  35\n",
      "Final 231.9791\n",
      "Task:  36\n",
      "Final 657.0131\n",
      "Task:  37\n",
      "Final -0.8757\n",
      "Task:  38\n",
      "Final 368.6394\n",
      "Task:  39\n",
      "Final 131.3454\n",
      "Task:  40\n",
      "Final 312.0689\n",
      "Task:  41\n",
      "Final 161.836\n",
      "Task:  42\n",
      "Final 87.3709\n",
      "Task:  43\n",
      "Final -3.1377\n",
      "Task:  44\n",
      "Final 226.4426\n",
      "Task:  45\n",
      "Final 359.778\n",
      "Task:  46\n",
      "Final 31.194\n",
      "Task:  47\n",
      "Final 401.9493\n",
      "Task:  48\n",
      "Final 4.4469\n",
      "Task:  49\n",
      "Final 281.6742\n",
      "Task:  50\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(10, 10, 3, 10)\n",
    "a = SFQL(env)\n",
    "a.algorithm(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zx13n_rYeuPR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVZ8o-xT-88I"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SFQL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
