{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8qeuYjM5-87z"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gLey0OjT-87-"
   },
   "outputs": [],
   "source": [
    "##Environment\n",
    "class GridWorld:\n",
    "    def __init__(self, height = 50, width = 50, n_c = 1, n_o = 1):\n",
    "        self.wd = width\n",
    "        self.ht = height\n",
    "        self.n_c = n_c            # no. of classes\n",
    "        self.n_o = n_o            # no. of objects  (n_c <= n_o)\n",
    "        self.reward = [-1, 1]     # range of rewards\n",
    "        self.start_coord = (0, 0)\n",
    "        self.goal_coord = (height-1, width-1)\n",
    "        self.x_g = height-1        # goal x-coord\n",
    "        self.x_g = width-1         # goal y-coord\n",
    "        self.r_g = 1               # goal reward\n",
    "        self.D = 100 + n_o + 1\n",
    "        self.d = n_c + 1\n",
    "        '''\n",
    "        UP: 0\n",
    "        RIGHT: 1\n",
    "        DOWN: 2\n",
    "        LEFT: 3\n",
    "        '''\n",
    "        self.actions = [0, 1, 2, 3]\n",
    "        #self.x_s, self.y_s = self.start_coord\n",
    "\n",
    "        self.object_coord = []\n",
    "        self.object_class = []\n",
    "        self.class_reward = {}\n",
    "        self.objects_so_far = [0] * self.n_o\n",
    "\n",
    "        self.generate_objects()\n",
    "        self.assign_classes()\n",
    "        self.generate_class_reward()\n",
    "\n",
    "    def generate_objects(self):\n",
    "        n_o = self.n_o\n",
    "        while n_o:\n",
    "            object_x = np.random.randint(self.wd)\n",
    "            object_y = np.random.randint(self.ht)\n",
    "            if((object_x,object_y) not in self.object_coord):\n",
    "                self.object_coord.append((object_x,object_y))\n",
    "                n_o -= 1\n",
    "  \n",
    "    def assign_classes (self):\n",
    "        self.object_class = np.random.randint(0, self.n_c, size = self.n_o)\n",
    " \n",
    "    def generate_class_reward(self):\n",
    "        for i in range(0, self.n_c):\n",
    "            self.class_reward[i] = np.random.uniform(self.reward[0], self.reward[1])\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_pos_x, self.cur_pos_y = self.start_coord\n",
    "        return self.start_coord\n",
    "\n",
    "    def render(self):\n",
    "        for i in range(self.wd):\n",
    "            for j in range(self.ht):\n",
    "                if (i, j) in self.object_coord:\n",
    "                    print(self.object_class[self.object_coord.index((i, j))], end = \"\")\n",
    "                else:\n",
    "                    print(\"*\",end = \"\")\n",
    "            print(\"\")\n",
    "            \n",
    "    def show_class_rewards(self):\n",
    "        print(self.class_reward)\n",
    "    \n",
    "    def restart(self):\n",
    "        self.cur_pos_x, self.cur_pos_y = 0, 0\n",
    "        self.objects_so_far = [0] * self.n_o\n",
    "        \n",
    "    def step(self, action):\n",
    "        flag = 1\n",
    "        reward = 0\n",
    "        phi_at_step = [0] * (self.n_c + 1)\n",
    "        done = False\n",
    "        if action == 0 and self.cur_pos_y + 1 < self.ht:\n",
    "            self.cur_pos_y += 1\n",
    "        elif action == 1 and self.cur_pos_x + 1 < self.wd:\n",
    "            self.cur_pos_x += 1\n",
    "        elif action == 2 and self.cur_pos_y - 1 >=0:\n",
    "            self.cur_pos_y -= 1\n",
    "        elif action == 3 and self.cur_pos_x - 1 >=0:\n",
    "            self.cur_pos_x -= 1\n",
    "        \n",
    "        pos = (self.cur_pos_x, self.cur_pos_y)\n",
    "        if pos == self.goal_coord:\n",
    "            phi_at_step[-1] = self.r_g\n",
    "            done = True\n",
    "            reward += 1\n",
    "        elif pos in self.object_coord:\n",
    "            object_index = self.object_coord.index(pos)\n",
    "            if self.objects_so_far[object_index] == 0:\n",
    "                phi_at_step[self.object_class[object_index]] = 1\n",
    "                self.objects_so_far[object_index] = 1\n",
    "                reward += self.class_reward[self.object_class[object_index]]\n",
    "\n",
    "        return pos, phi_at_step, reward, done\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld(10, 10, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**1*****0*\n",
      "*********0\n",
      "***0******\n",
      "**0*******\n",
      "**********\n",
      "*1********\n",
      "**1*******\n",
      "*****1****\n",
      "*********0\n",
      "*******1**\n",
      "State (3, 2) PHI [1, 0, 0] Reward 0.2117659292564884 State (0, 2) PHI [0, 1, 0] Reward -0.2208204933185507 State (2, 3) PHI [1, 0, 0] Reward 0.2117659292564884 State (8, 9) PHI [1, 0, 0] Reward 0.2117659292564884 State (9, 7) PHI [0, 1, 0] Reward -0.2208204933185507 State (9, 9) PHI [0, 0, 1] Reward 1 "
     ]
    }
   ],
   "source": [
    "done = False\n",
    "state = env.restart()\n",
    "env.render()\n",
    "#env.showClassRewards()\n",
    "while not done:\n",
    "    a = np.random.choice([0, 1, 2, 3])\n",
    "    state, phi, r, done = env.step(a)\n",
    "    if r != 0:\n",
    "        print(\"State\", state, end = \" \")\n",
    "        print(\"PHI\", phi, end = \" \")\n",
    "        print(\"Reward\", r, end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zex7rD5HevzT"
   },
   "outputs": [],
   "source": [
    "class RadialBasis():\n",
    "\n",
    "    def __init__(self, x_dim, y_dim, basis_x, basis_y):\n",
    "\n",
    "        self.centres = []\n",
    "        self.x_dim, self.y_dim = x_dim, y_dim\n",
    "        for x in range(0, x_dim, int(x_dim/basis_x)):\n",
    "            for y in range(0, y_dim, int(y_dim/basis_y)):\n",
    "                self.centres.append((x/x_dim, y/y_dim))\n",
    "      \n",
    "    def getPositionVector(self, x, y):\n",
    "        state = []\n",
    "        x,y = x/self.x_dim, y/self.y_dim\n",
    "        for cx, cy in self.centres:\n",
    "            state.append(np.exp(-1 * ((cx - x)**2 + (cy - y)**2)/0.1))\n",
    "        return np.array(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OMwb80ZW-88E"
   },
   "outputs": [],
   "source": [
    "class SFQL:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.reward_weight_list = []\n",
    "        self.Z_list = []\n",
    "        self.eps_greedy = 0.7\n",
    "        self.w_alpha = 0.01\n",
    "        self.z_alpha = 0.01\n",
    "        self.w_err_th = 0.01\n",
    "        self.rdb = RadialBasis(env.wd, env.ht, 10, 10)\n",
    "\n",
    "    def SGD(self, alpha, w_t, reward, object_at_step):\n",
    "        def cost(reward, w_t, object_at_step):\n",
    "            return np.square(reward - np.dot(object_at_step.T, w_t))\n",
    "  \n",
    "        while cost(w_t, phi) >= self.w_err_th:\n",
    "            w_t = w_t - alpha*np.dot((reward - np.dot(object_at_step.T,w_t)).T, object_at_step)\n",
    "\n",
    "    def featurize_state(self, state):\n",
    "        return np.concatenate(self.rdb.getPosititionVector(state[0], state[1]), self.env.objects_so_far)\n",
    "    \n",
    "    def find_best_psi(self, w_t, state):\n",
    "        max_k = 0\n",
    "        action_val = -999999\n",
    "        action_val_his = -999999\n",
    "        for k in range(0,self.psi_list):\n",
    "            for action in self.env.actions: \n",
    "                psi = np.dot(self.featurize_state(state).T,Z[k][action])\n",
    "                new_action_val = np.dot(psi.T,w_t)\n",
    "                action_val = max(action_val,new_action_val)\n",
    "            max_k = k if (action_val > action_val_his) else max_k\n",
    "            action_val_his = action_val\n",
    "        \n",
    "        return max_k \n",
    "                \n",
    "    def get_action(self,state,c,w_t):\n",
    "        if np.random.uniform(0,1) < self.eps_greedy:   #In paper, Bernoulli is considered imstead of uniform\n",
    "            return np.random.choice(self.env.actions)\n",
    "        else:\n",
    "            val_his = -99999\n",
    "            act_choice = 0\n",
    "            for action in self.env.actions:\n",
    "                psi = np.dot(self.featurize_state(state).T,Z[c][action])\n",
    "                val = np.dot(psi.T,w_t)\n",
    "                act_choice = action if val > val_his else act_choice\n",
    "                val_his = max(val_his,val)\n",
    "            return act_choice\n",
    "\n",
    "    def algorithm(self,num_tasks):\n",
    "        D = self.env.D\n",
    "        d = self.env.d\n",
    "\n",
    "        NUM_EPISODES = 100\n",
    "        \n",
    "        #TODO: How to initialize the list? add before train or after train (only for the first task)\n",
    "        \n",
    "        Z = [np.random.rand(self.D,self.d) for r in range(len(env.self.actions))]\n",
    "  \n",
    "        for t in range(0, num_tasks):\n",
    "            env.generate_class_reward()\n",
    "            w_t = np.random.rand(self.d)\n",
    "\n",
    "            for ep in range(NUM_EPISODES):\n",
    "                state = self.env.reset()\n",
    "\n",
    "                while not done:\n",
    "                    c = self.find_best_psi(w_t,state)\n",
    "                    w_c = self.reward_weight_list[c]\n",
    "                    action = self.get_action(state,c,w_t)\n",
    "                    pos,object_at_step,reward,done = self.env.step(action)\n",
    "                    if done:\n",
    "                        gamma = 0\n",
    "                        new_episode = True\n",
    "                    else:\n",
    "                        c_prime = self.find_best_psi(w_t,pos)\n",
    "                        a_prime = self.get_action(pos,c_prime,w_t)\n",
    "\n",
    "                    self.SGD(self.w_alpha,w_t,reward,object_at_step)\n",
    "\n",
    "                    psi_prime = np.dot(self.featurize_state(pos).T,Z[t][a_prime])\n",
    "                    #psi = np.dot(self.featurize_state(state).T,Z[t][action])\n",
    "                    z = Z[t][action]\n",
    "                    for k in range(0,self.d):\n",
    "                        target = object_at_step[k] + gamma*psi_prime[k]\n",
    "                        self.SGD(self.z_alpha,z[:,k],target,self.featurize_state(state))\n",
    "\n",
    "                    if c is not t:\n",
    "                        a_prime = self.get_action(pos,c,w_c)\n",
    "\n",
    "                        psi_prime = np.dot(self.featurize_state(pos).T,Z[c][a_prime])\n",
    "                        #psi = np.dot(self.featurize_state(state).T,Z[t][action])\n",
    "                        z = Z[c][action]\n",
    "                        for k in range(0,self.d):\n",
    "                            target = object_at_step[k] + gamma*psi_prime[k]\n",
    "                            self.SGD(self.z_alpha,z[:,k],target,self.featurize_state(state))\n",
    "\n",
    "                    state = obs\n",
    "                    \n",
    "                    \n",
    "        #before moving to next task, save w_t\n",
    "        self.reward_weight_list.append(w_t)\n",
    "        self.Z_list.append(Z)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LsCH3-2ueuQd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zx13n_rYeuPR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVZ8o-xT-88I"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SFQL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
